{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- 🤝 Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- 🤝 Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# 🤝 Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using our Loan Data once again - this time the strutured data available through the CSV!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "loader = CSVLoader(\n",
        "    file_path=f\"./data/complaints.csv\",\n",
        "    metadata_columns=[\n",
        "      \"Date received\", \n",
        "      \"Product\", \n",
        "      \"Sub-product\", \n",
        "      \"Issue\", \n",
        "      \"Sub-issue\", \n",
        "      \"Consumer complaint narrative\", \n",
        "      \"Company public response\", \n",
        "      \"Company\", \n",
        "      \"State\", \n",
        "      \"ZIP code\", \n",
        "      \"Tags\", \n",
        "      \"Consumer consent provided?\", \n",
        "      \"Submitted via\", \n",
        "      \"Date sent to company\", \n",
        "      \"Company response to consumer\", \n",
        "      \"Timely response?\", \n",
        "      \"Consumer disputed?\", \n",
        "      \"Complaint ID\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "loan_complaint_data = loader.load()\n",
        "\n",
        "for doc in loan_complaint_data:\n",
        "    doc.page_content = doc.metadata[\"Consumer complaint narrative\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': './data/complaints.csv', 'row': 0, 'Date received': '03/27/25', 'Product': 'Student loan', 'Sub-product': 'Federal student loan servicing', 'Issue': 'Dealing with your lender or servicer', 'Sub-issue': 'Trouble with how payments are being handled', 'Consumer complaint narrative': \"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\", 'Company public response': 'None', 'Company': 'Nelnet, Inc.', 'State': 'IL', 'ZIP code': '60030', 'Tags': 'None', 'Consumer consent provided?': 'Consent provided', 'Submitted via': 'Web', 'Date sent to company': '03/27/25', 'Company response to consumer': 'Closed with explanation', 'Timely response?': 'Yes', 'Consumer disputed?': 'N/A', 'Complaint ID': '12686613'}, page_content=\"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loan_complaint_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"LoanComplaints\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    loan_complaint_data,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"LoanComplaints\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, one of the most common issues with loans appears to be problems related to mismanagement and errors by loan servicers. This includes errors in loan balances, misapplied payments, wrongful denials of payment plans, incorrect reporting of account status, and difficulties in applying payments correctly. Many complaints also involve discrepancies in loan balances, incorrect or outdated information on credit reports, unauthorized transfers of loans without proper notification, and trouble with loan information and repayment terms.\\n\\nTherefore, the most common issue seems to be **problems caused by errors, mismanagement, or mishandling by loan servicers**, which often lead to inaccurate loan information, credit report inaccuracies, and repayment difficulties for borrowers.'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, it appears that several complaints were not handled in a timely manner. For example:\\n\\n- The complaint received on 03/28/25 from MOHELA was marked as **\"Timely response?\": No**, indicating it was not handled promptly.\\n- The complaint received on 04/14/25 from Nelnet, Inc. also was marked as **\"Timely response?\": Yes**, so it was handled promptly.\\n- The complaint received on 04/24/25 from Maximus Federal Services, Inc. was also marked as **\"Timely response?\": Yes**.\\n- However, the complaint received on 04/24/25 from Nelnet, Inc. was handled in a timely manner as well.\\n- The complaint from 04/18/25 from EdFinancial Services was handled promptly.\\n- The complaint received on 04/05/25 from Maximus Federal Services, Inc. was marked as **\"Timely response?\": Yes**.\\n- The complaint from 04/14/25 from EdFinancial Services was handled promptly.\\n\\nBut the complaint from 03/28/25 from MOHELA was explicitly not handled in time, which indicates at least one complaint was delayed.\\n\\n**Conclusion:**\\nYes, at least one complaint (from MOHELA on 03/28/25) was not handled in a timely manner.'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans mainly because of financial hardships arising from unmanageable interest accumulation, miscommunication or lack of proper information from lenders or servicers, and the inability to afford increased payments. Many borrowers experienced complications such as interest continuously accruing despite making payments, being placed in forbearance or deferment options that extended repayment periods and increased total interest owed, and not being properly notified about their loan status or payment schedules. Additionally, some borrowers faced issues like transfers between loan servicers without proper communication, incorrect or confusing information about repayment obligations, or difficulties in applying extra payments towards principal. These factors, combined with stagnant wages and economic challenges, made timely loan repayment difficult or impossible for many, leading to delinquency or default.'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(loan_complaint_data, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issue with loans appears to be problems with dealing with lenders or servicers, including issues like incorrect or bad information about loans, difficulty applying payments correctly, and disputes over fees or loan details. Specifically, complaints highlight issues such as:\\n\\n- Disputes over fees charged or incorrect billing\\n- Trouble with how payments are applied, often in ways that are unfavorable to borrowers\\n- Receiving inaccurate or incomplete information about loan balances or terms\\n- Problems with loan repayment terms and understanding of interest calculations\\n\\nTherefore, the most common issue with loans, as reflected in the complaints, is difficulties in communication and trust with lenders or servicers, including mismanagement, misinformation, and predatory practices.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, it appears that several complaints were addressed with a response from the companies, and all responses indicate that they were handled in a timely manner. Specifically, the complaints received responses marked as \"Yes\" for \"Timely response?\" from the companies involved. \\n\\nHowever, there are complaints where the consumer expressed ongoing frustration, particularly regarding issues with account corrections and responses, but there is no indication that these unresolved issues were left unhandled entirely. \\n\\nTherefore, according to the provided data, no complaints were explicitly stated as not being handled in a timely manner.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People often fail to pay back their loans due to various issues such as miscommunication, improper management by lenders or servicers, and lack of clarity about their loan status. For example, some borrowers experienced automatic payments being canceled or not received due to errors or lack of proper notification, leading to late or missed payments. Others struggled because they were not informed about loan transfers, repayment suspensions, or changes in autopay arrangements, which resulted in their accounts being marked as overdue and negatively impacting their credit scores. Additionally, delayed or insufficient responses from loan servicing companies and difficulties in navigating payment plans or forbearance options contribute to repayment failures.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "#### ❓ Question #1:\n",
        "\n",
        "Give an example query where BM25 is better than embeddings and justify your answer.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "### Answer:\n",
        "\n",
        "Since Best-Marchin 25 relies on the frequency of terms, then it's strength is in exact matching. In my queries, I gave specific terms which would aid in retrieval.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issue with loans appears to be problems related to dealing with lenders or servicers, including errors in loan balances, misapplied payments, wrongful denials of payment plans, incorrect or inconsistent loan information, and improper handling of loan data. Many complaints involve incorrect or confusing information about loan balances, interest, and payment requirements, as well as issues with communication and documentation.\\n\\nIf I had to sum it up, a prevalent issue is **mismanagement and miscommunication by loan servicers**, leading to errors in account information, unpaid balances, and disputes over loan handling.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, yes, there are complaints that did not get handled in a timely manner. For example, one consumer\\'s complaint regarding the status of their loan account has been open for over 18 months with no resolution, and they are still awaiting responses and account adjustments. Additionally, another complaint regarding the issue of payments not being applied to their student loan account was not resolved promptly. Although the company responses to these complaints were marked as \"Closed with explanation\" and responses were timely, the complaints themselves remained unresolved for an extended period, indicating that some issues were not addressed in a timely manner.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for several reasons indicated in the complaints:\\n\\n1. **Lack of Understanding and Miscommunication:** Borrowers were often unaware that they were required to repay their loans or did not receive clear information about the repayment obligations, interest accumulation, or loan terms. For example, one individual was unaware they had to repay their student aid until years later because they were not informed by financial aid officers.\\n\\n2. **Administrative Issues and Poor Communication:** Borrowers reported that loan servicers did not notify them when they bought out other lenders, failed to inform them about due dates, or did not provide proper notifications about payments or account status. This lack of communication made it difficult for borrowers to stay informed or manage their loans effectively.\\n\\n3. **Interest Accumulation and Loan Management Options:** Although options like forbearance or deferment were available, interest continued to accrue during these periods, increasing the total amount owed. Borrowers felt trapped because reducing payments could lead to interest snowballing, extending repayment periods and increasing total debt.\\n\\n4. **Difficulty Affording Payments:** Many borrowers could not afford increased or standard payments due to financial hardship, stagnating wages, or economic conditions. Some felt the available payment options (such as forbearance or deferment) made repayment impossible due to growing interest, making it harder to pay off their loans.\\n\\n5. **Inadequate or Complex Loan Information:** Borrowers did not receive detailed breakdowns of interest, loan balances, or the reasons for discrepancies in their account statements, leading to confusion and a feeling of being misled.\\n\\nIn summary, failure to pay back loans was often due to a combination of insufficient communication, misunderstanding of repayment obligations, the compounding interest during deferment, and financial hardships that limited the ability to make payments.'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issues with student loans, based on the complaints provided, tend to involve:\\n\\n- Mismanagement by servicers, including errors in loan balances and interest calculations.\\n- Difficulty in obtaining accurate or clear information about loan terms, balances, or forgiveness options.\\n- Problems with loan transfers and lack of proper notification.\\n- Unauthorized or incorrect reporting to credit bureaus.\\n- Challenges with repayment plans, such as being steered into forbearance with accruing interest, or inability to apply extra payments to principal.\\n- Mishandling of data breaches or privacy violations.\\n- Issues with loan forgiveness, cancellation, or discharge processes.\\n\\nIn summary, the most common issue appears to be **mismanagement and miscommunication by loan servicers**, leading to errors, lack of transparency, and difficulties for borrowers trying to manage or resolve their student loans.'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, yes, some complaints indicate that complaints were not handled in a timely manner. Specifically:\\n\\n- One complaint (Complaint #12709087) about a delay in processing a graduated loan application, where the complaint response was \"Not timely\" and the complaint was unresolved for over 15 days.\\n- Another complaint (Complaint #12739706) about failure to inform about delinquent loans, which was marked \"Not timely\" and had been unresolved for over 18 months.\\n- Several complaints mention that the organizations failed to respond or resolve issues for extended periods, ranging from several months to over a year.\\n- Additionally, some complaints note that the companies\\' responses were \"Closed with explanation\" despite ongoing issues, indicating they may not have addressed the issues promptly.\\n\\nIn summary, multiple complaints suggest that certain issues were not handled in a timely manner by the organizations involved.'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans primarily due to a combination of financial hardships, lack of clear information, and systemic issues with loan servicing. Specific reasons include:\\n\\n1. **Accruing interest and repayment difficulties:** Borrowers often had limited options besides forbearance or deferment, which allowed interest to continue accumulating, making loans harder to pay off over time.\\n\\n2. **Lack of alternative repayment options:** Many borrowers were not informed about income-driven repayment plans, loan forgiveness programs, or rehabilitation options that could have made repayment more manageable.\\n\\n3. **Systemic mismanagement by servicers:** Problems such as improper handling of accounts, unauthorized transfers, and failure to follow federal regulations led to inaccurate reporting, defaults, and credit score drops.\\n\\n4. **Lack of sufficient income or job stability:** Borrowers faced stagnant wages, unemployment, or job loss, making it impossible to keep up with payments while covering basics like food and housing.\\n\\n5. **Misleading or insufficient information:** Borrowers reported being misled about their payment obligations, the effects of forbearance, and available options, leading to unintentional default or missed payments.\\n\\n6. **Servicing practices and coercion:** There are documented cases where borrowers were steered into long-term forbearances or forced into costly refinancing or consolidation without proper counseling, resulting in increased debt.\\n\\n7. **Compounding interest:** Many borrowers saw their loan balances grow over years due to unanticipated interest accumulation and capitalization, despite regular payments or efforts to manage their debts.\\n\\n8. **Inadequate communication and oversight:** Lack of proper notification, account management failures, and poor customer service contributed to borrowers being unaware of missed payments or account status changes.\\n\\nIn summary, the combination of economic hardship, systemic servicing failures, and lack of transparent, accessible options contributed to many people's inability to pay back their loans.\""
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "#### ❓ Question #2:\n",
        "\n",
        "Explain how generating multiple reformulations of a user query can improve recall.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "### Answer:\n",
        "- I think since reformulation can be done by paraphrasing, highlighting, or expounding the original user query, it provides a comprehensive coverage, and a deeper or more guided retrieval.\n",
        "- And then since reformulated each n queries retrieves k number of related documents, the chances of finding related documents for quality context increases.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = loan_complaint_data\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = QdrantVectorStore(\n",
        "    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the provided complaints, appears to be problems related to federal student loan servicing. Specific sub-issues include errors in loan balances, misapplied payments, wrongful denials of payment plans, and disputes over loan information such as interest rates and account status. Many complaints also involve misconduct from loan servicers, including errors, unfair practices, and issues with credit reporting and verification.\\n\\nIn summary, the most prevalent problem highlighted is misconduct or errors by loan servicers in managing federal student loans.'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, several complaints, including the one received on 03/28/25 and others on 04/11/25, indicate that some complaints were not handled in a timely manner. Specifically, the complaint dated 03/28/25 mentions that the consumer was told it would take 15 days for someone to reach out, but as of the date of the complaint, no one had contacted them yet. Similarly, the complaints from 04/11/25 regarding various issues with Mohela were also marked as \"No\" for timely response, which suggests they were not handled promptly. \\n\\nTherefore, yes, there were complaints that did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans mainly due to a lack of proper communication and understanding of their payment obligations, as well as financial hardship. In some cases, issues such as erroneous reporting, failure to notify borrowers about payment requirements, and the inability to access information about loan ownership or payment plans contributed to missed payments. Additionally, borrowers faced severe financial difficulties, such as inability to find employment or experiencing health issues, which made it difficult or impossible to keep up with loan repayments.'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, the most common issues with student loans include:\\n\\n1. Dealing with lenders or servicers, often involving errors in loan balances, misapplied payments, or wrongful denials of payment plans.\\n2. Problems with how payments are handled, such as restrictions on applying extra payments to principal or issues with payment application methods.\\n3. Discrepancies in reported loan status or balances, including inaccurate credit reporting and account status errors.\\n4. Bad information or misinformation about loan terms, interest, or repayment options.\\n5. Problems with loan management related to transfers, legal disputes, or improper documentation.\\n6. Struggles with repayment, including issues like interest accumulation, difficulty with repayment plans or forgiveness programs, and complications caused by loan servicing practices.\\n7. Privacy violations and improper handling of personal data during loan processes or transfers.\\n\\nWhile multiple issues are frequent, a prominent concern appears to be the mishandling of payments and account information, along with errors and inaccuracies in loan balances, statuses, and reporting. These issues often exacerbate borrowers’ financial hardships and cause confusion and frustration.\\n\\nTherefore, the most common issue with loans, especially as reflected in the complaints data, is **problems related to how loans are managed by servicers, including errors in balances, misapplied payments, and inaccurate reporting**, leading to borrower confusion, financial harm, and difficulties in repayment.\\n\\nIf you have any more questions or need further assistance, please let me know!'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, based on the provided complaints, some complaints did not get handled in a timely manner. For example, the complaint with ID \\'12739706\\' from MOHELA filed on 04/01/25 was marked as \"Timely response?\": \"No,\" indicating it was not responded to in a timely manner. Additionally, several other complaints with statuses such as \"Closed with explanation\" or \"Non-response\" suggest delays or lack of timely handling.'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans primarily due to a combination of factors highlighted in the complaints:\\n\\n1. **Lack of Clear Communication and Notification:** Many borrowers reported not receiving timely or any notices about their loan status, repayment start dates, changes in servicers, or default warnings. For example, some loans were transferred to new servicers without proper notification, leading borrowers to be unaware that repayment had resumed or that their account was delinquent.\\n\\n2. **Compounding and Unmanageable Interest:** Borrowers expressed that interest continued to accrue and capitalize, increasing their total debt over time. Many were misled about how interest would grow, especially during forbearance or deferment, which often resulted in higher balances that became difficult or impossible to pay off.\\n\\n3. **Mismanagement and Errors by Servicers:** Complaints included incorrect reporting of delinquency, inaccurate account balances, or failure to process forbearance or income-driven repayment plans effectively. This mismanagement sometimes led to erroneous late payments or default statuses being placed on their credit reports.\\n\\n4. **Inadequate Support and Supportive Programs:** Several borrowers mentioned that servicers did not offer or inform them about available income-driven repayment plans, loan forgiveness, or rehabilitation options. As a result, many remained stuck in unaffordable payment plans or default, especially when they faced financial hardships like job loss or medical issues.\\n\\n5. **Administrative and Technical Problems:** Frequent issues such as payments being reversed, not being notified of loan transfers, or accounts being incorrectly reported as delinquent contributed to borrowers’ inability to maintain or resume payments.\\n\\n6. **Financial Hardships and Life Circumstances:** Economic challenges such as unemployment, illness, homelessness, or other hardships made it difficult to keep up with payments, especially when compounded by high interest and lack of flexible repayment options.\\n\\nIn summary, the failure to pay back loans was often not due to irresponsibility, but rather systemic issues, poor communication, misunderstandings about loan terms, and insufficient support from loan servicers, all of which made managing repayment challenging or impossible for many borrowers.'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(loan_complaint_data[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Loan_Complaint_Data_Semantic_Chunks\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the information provided, a common issue with loans appears to be problems related to loan servicing and administrative errors. Specific frequent issues include:\\n\\n- Struggling to repay loans or problems with forgiveness, cancellation, or discharge.\\n- Improper reporting or use of credit reports.\\n- Issues with loan payments, such as auto-debit problems, incorrect payment amounts, or delays in re-amortization after forbearance.\\n- Discrepancies or errors in loan account statuses, such as mistakenly being reported in default or delinquency despite never being in such status.\\n- Difficulties in obtaining clear information about loan balances, servicer changes, or loan terms.\\n- Unauthorized access or breaches related to personal and financial information.\\n\\nOverall, the most common issue appears to be administrative mistakes or miscommunications by loan servicers, leading to hardships in repayment, incorrect reporting, and challenges in managing loan accounts.'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, it appears that most complaints received timely responses, but many involve issues with payments, account corrections, or disputes that may take time to resolve. \\n\\nSpecifically, one complaint (row 14) indicates that the complaint was handled with a \"Closed with explanation\" response and a \"Yes\" for timely response. The other complaints also received responses within the expected time frames, but some involve ongoing disputes or complicated issues like breaches of contract or unlawful data reporting, which may not have been fully resolved yet.\\n\\nTherefore, from the information given, it does not clearly indicate that any complaints were not handled in a timely manner. However, some complaints involve complex issues that might still be unresolved, but the data provided does not specify delays or failures to respond in time.\\n\\n**In conclusion:** No, there is no explicit evidence in the provided data that any complaints were not handled in a timely manner.'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for various reasons, including difficulties with loan management, issues with documentation and verification, disputes over the legitimacy of debts, problems with the reporting and handling of their loans, and errors or delays caused by loan servicers. Some specific issues highlighted include lack of clarity and transparency from lenders, poor communication, delays in re-amortizing payments after forbearance, and potentially illegal or improper reporting of debts. Additionally, some borrowers faced challenges due to alleged mismanagement or breaches of privacy and contract law by loan servicers, which further complicated their ability to repay or resolve their loan issues.'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "#### ❓ Question #3:\n",
        "\n",
        "If sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "### Answer:\n",
        "\n",
        "Its behavior would be generating small chunks and the produced embedded vectors are similar to each other. As a result, a retrieval would return repetitive unhelpful documents.\n",
        "\n",
        "If I were to adjust anything, I would increase the breakpoint_threshold_amount and/or min_chunk_size, which are the parameters of SemanticChunker based on the given website. Other than that, if possible, combining each Q and A pair and adding metadata (e.g. tag, company) might help.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# 🤝 Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API key found? True\n",
            "Tracing v2 enabled? true\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "# Import Ragas components\n",
        "from ragas.testset import TestsetGenerator\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    context_precision,\n",
        "    context_recall,\n",
        "    answer_relevancy,\n",
        "    faithfulness,\n",
        "    answer_correctness\n",
        ")\n",
        "\n",
        "# Import LangSmith for tracking\n",
        "from langchain.callbacks import LangChainTracer\n",
        "from langsmith import Client\n",
        "\n",
        "# Set up LangSmith tracking\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LangChain API Key:\")\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Retrievers Eval\"\n",
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")\n",
        "\n",
        "import os\n",
        "\n",
        "print(\"API key found?\" , bool(os.environ.get(\"LANGCHAIN_API_KEY\")))\n",
        "print(\"Tracing v2 enabled?\" , os.environ.get(\"LANGCHAIN_TRACING_V2\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: Creating Golden Dataset...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9d458bd57f84f13b11e65dddd6e3fcd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c6668ee7a664304b8865336c9681f77",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bb6259bc6e14ff8b4ceea4f4f483526",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/45 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df7ef27ac9bc44cbbbd7212f1c76d29f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5cbafcf48468459189d619c6b8201dc1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de08ff0bf1704ce38c52fb158edc0a4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2fb8b9fb9dec494d8c203b896d0b31ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/21 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 21 test cases\n",
            "\n",
            "Step 2: Setting up retrievers...\n"
          ]
        }
      ],
      "source": [
        "client = Client()\n",
        "\n",
        "# Your existing imports and setup\n",
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from operator import itemgetter\n",
        "\n",
        "# Load your documents (using your existing code)\n",
        "path = \"bills/\"\n",
        "loader = DirectoryLoader(path, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
        "docs = loader.load()\n",
        "\n",
        "# Step 1: Create Golden Dataset (Enhanced version of your approach)\n",
        "print(\"Step 1: Creating Golden Dataset...\")\n",
        "\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\", temperature=0))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
        "\n",
        "# Generate test dataset\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "\n",
        "# Use a subset for cost management\n",
        "test_docs = docs[:15]\n",
        "dataset = generator.generate_with_langchain_docs(\n",
        "    test_docs, \n",
        "    testset_size=20,  # Increased for better evaluation\n",
        "    with_debugging_logs=True\n",
        ")\n",
        "\n",
        "# Convert to DataFrame and save\n",
        "test_df = dataset.to_pandas()\n",
        "test_df.to_csv('bills_evaluation_dataset.csv', index=False)\n",
        "print(f\"Generated {len(test_df)} test cases\")\n",
        "\n",
        "# Step 2: Set up all retrievers (using your bills data)\n",
        "print(\"\\nStep 2: Setting up retrievers...\")\n",
        "\n",
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain.retrievers import ParentDocumentRetriever, EnsembleRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "\n",
        "# Initialize embeddings and chat model\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "chat_model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# Create base vectorstore\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    test_docs,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"BillsComplaints\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Naive Retriever\n",
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
        "\n",
        "# 2. BM25 Retriever\n",
        "bm25_retriever = BM25Retriever.from_documents(test_docs, k=10)\n",
        "\n",
        "# 3. Contextual Compression Retriever (with Cohere Rerank)\n",
        "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, \n",
        "    base_retriever=naive_retriever\n",
        ")\n",
        "\n",
        "# 4. Multi-Query Retriever\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, \n",
        "    llm=chat_model,\n",
        "    include_original=True\n",
        ")\n",
        "\n",
        "# 5. Parent Document Retriever\n",
        "client_parent = QdrantClient(location=\":memory:\")\n",
        "client_parent.create_collection(\n",
        "    collection_name=\"parent_docs\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_vectorstore = QdrantVectorStore(\n",
        "    collection_name=\"parent_docs\", \n",
        "    embedding=embeddings, \n",
        "    client=client_parent\n",
        ")\n",
        "\n",
        "store = InMemoryStore()\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore=parent_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")\n",
        "parent_document_retriever.add_documents(test_docs)\n",
        "\n",
        "# 6. Ensemble Retriever\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[naive_retriever, bm25_retriever],\n",
        "    weights=[0.5, 0.5]\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 3: Creating RAG chains...\n",
            "\n",
            "Step 4: Starting evaluation...\n"
          ]
        }
      ],
      "source": [
        "# Define retrievers to evaluate\n",
        "retrievers = {\n",
        "    \"Naive\": naive_retriever,\n",
        "    \"BM25\": bm25_retriever,\n",
        "    \"Contextual_Compression\": compression_retriever,\n",
        "    \"Multi_Query\": multi_query_retriever,\n",
        "    \"Parent_Document\": parent_document_retriever,\n",
        "    \"Ensemble\": ensemble_retriever\n",
        "}\n",
        "\n",
        "# Step 3: Create RAG chains for each retriever\n",
        "print(\"\\nStep 3: Creating RAG chains...\")\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"You are a helpful assistant. Use the context provided below to answer the question accurately.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Context: {context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
        "\n",
        "def create_rag_chain(retriever):\n",
        "    \"\"\"Create a RAG chain with the given retriever\"\"\"\n",
        "    return (\n",
        "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "        | {\"response\": rag_prompt | chat_model | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
        "    )\n",
        "\n",
        "# Create chains for each retriever\n",
        "rag_chains = {name: create_rag_chain(retriever) for name, retriever in retrievers.items()}\n",
        "\n",
        "# Step 4: Evaluate each retriever\n",
        "print(\"\\nStep 4: Starting evaluation...\")\n",
        "\n",
        "def format_docs(docs):\n",
        "    \"\"\"Format documents for Ragas evaluation\"\"\"\n",
        "    if isinstance(docs, list):\n",
        "        return [doc.page_content if hasattr(doc, 'page_content') else str(doc) for doc in docs]\n",
        "    return [str(docs)]\n",
        "\n",
        "# Metrics to evaluate\n",
        "metrics = [\n",
        "    context_precision,\n",
        "    context_recall,\n",
        "    answer_relevancy,\n",
        "    faithfulness,\n",
        "    answer_correctness\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating Naive...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "add3093b3f414d9eb3fe4d3833c3012f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/105 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Convert to Ragas dataset format\u001b[39;00m\n\u001b[32m     53\u001b[39m ragas_dataset = Dataset.from_dict(eval_data)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m ragas_result = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mragas_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Now using HuggingFace Dataset\u001b[39;49;00m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator_llm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator_embeddings\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m results[retriever_name] = {\n\u001b[32m     63\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mscores\u001b[39m\u001b[33m\"\u001b[39m: ragas_result,\n\u001b[32m     64\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mavg_latency\u001b[39m\u001b[33m\"\u001b[39m: avg_latency\n\u001b[32m     65\u001b[39m }\n\u001b[32m     67\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretriever_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Average Latency: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_latency\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/psi-ai/day_5/.venv/lib/python3.13/site-packages/ragas/_analytics.py:227\u001b[39m, in \u001b[36mtrack_was_completed.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> t.Any:\n\u001b[32m    226\u001b[39m     track(IsCompleteEvent(event_type=func.\u001b[34m__name__\u001b[39m, is_completed=\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m     track(IsCompleteEvent(event_type=func.\u001b[34m__name__\u001b[39m, is_completed=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/psi-ai/day_5/.venv/lib/python3.13/site-packages/ragas/evaluation.py:294\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(dataset, metrics, llm, embeddings, experiment_name, callbacks, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar)\u001b[39m\n\u001b[32m    291\u001b[39m scores: t.List[t.Dict[\u001b[38;5;28mstr\u001b[39m, t.Any]] = []\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# get the results\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     results = \u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    295\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m results == []:\n\u001b[32m    296\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/psi-ai/day_5/.venv/lib/python3.13/site-packages/ragas/executor.py:213\u001b[39m, in \u001b[36mExecutor.results\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    210\u001b[39m             nest_asyncio.apply()\n\u001b[32m    211\u001b[39m             \u001b[38;5;28mself\u001b[39m._nest_asyncio_applied = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m results = \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_jobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m sorted_results = \u001b[38;5;28msorted\u001b[39m(results, key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m0\u001b[39m])\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [r[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m sorted_results]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/psi-ai/day_5/.venv/lib/python3.13/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/psi-ai/day_5/.venv/lib/python3.13/site-packages/nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/psi-ai/day_5/.venv/lib/python3.13/site-packages/nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.5-macos-aarch64-none/lib/python3.13/selectors.py:548\u001b[39m, in \u001b[36mKqueueSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    546\u001b[39m ready = []\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     kev_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    550\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[35]: AttributeError('NoneType' object has no attribute 'generate')\n",
            "Exception raised in Job[52]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[53]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[30]: AttributeError('NoneType' object has no attribute 'generate')\n",
            "Exception raised in Job[40]: AttributeError('NoneType' object has no attribute 'generate')\n",
            "Exception raised in Job[45]: AttributeError('NoneType' object has no attribute 'generate')\n",
            "Exception raised in Job[50]: AttributeError('NoneType' object has no attribute 'generate')\n",
            "Exception raised in Job[48]: AssertionError(llm must be set to compute score)\n",
            "Exception raised in Job[54]: AssertionError(LLM must be set)\n",
            "Exception raised in Job[55]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[56]: AssertionError(set LLM before use)\n",
            "Exception raised in Job[57]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[58]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[59]: AssertionError(LLM must be set)\n",
            "Exception raised in Job[60]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[61]: AssertionError(set LLM before use)\n",
            "Exception raised in Job[62]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[63]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[64]: AssertionError(LLM must be set)\n",
            "Exception raised in Job[65]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[66]: AssertionError(set LLM before use)\n",
            "Exception raised in Job[67]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[68]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[69]: AssertionError(LLM must be set)\n",
            "Exception raised in Job[70]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[71]: AssertionError(set LLM before use)\n",
            "Exception raised in Job[72]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[73]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[74]: AssertionError(LLM must be set)\n",
            "Exception raised in Job[75]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[76]: AssertionError(set LLM before use)\n",
            "Exception raised in Job[77]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[78]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[79]: AssertionError(LLM must be set)\n",
            "Exception raised in Job[80]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[81]: AssertionError(set LLM before use)\n",
            "Exception raised in Job[82]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[83]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[84]: AssertionError(LLM must be set)\n",
            "Exception raised in Job[85]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[86]: AssertionError(set LLM before use)\n",
            "Exception raised in Job[87]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[88]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[89]: AssertionError(LLM must be set)\n",
            "Exception raised in Job[90]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[91]: AssertionError(set LLM before use)\n",
            "Exception raised in Job[92]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[93]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[94]: AssertionError(LLM must be set)\n",
            "Exception raised in Job[95]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[96]: AssertionError(set LLM before use)\n",
            "Exception raised in Job[97]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[98]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[99]: AssertionError(LLM must be set)\n",
            "Exception raised in Job[100]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[101]: AssertionError(set LLM before use)\n",
            "Exception raised in Job[102]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[103]: AssertionError(LLM is not set)\n",
            "Exception raised in Job[104]: AssertionError(LLM must be set)\n",
            "Exception raised in Job[49]: AssertionError(llm is not set)\n",
            "Exception raised in Job[44]: AssertionError(llm is not set)\n",
            "Exception raised in Job[43]: AssertionError(llm must be set to compute score)\n",
            "Exception raised in Job[29]: AssertionError(AnswerSimilarity must be set)\n",
            "Exception raised in Job[34]: AssertionError(AnswerSimilarity must be set)\n",
            "Exception raised in Job[39]: AssertionError(AnswerSimilarity must be set)\n"
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "cost_tracking = {}\n",
        "latency_tracking = {}\n",
        "\n",
        "\n",
        "for retriever_name, chain in rag_chains.items():\n",
        "    print(f\"\\nEvaluating {retriever_name}...\")\n",
        "    \n",
        "    # Track costs and latency\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Prepare data for evaluation\n",
        "    questions = test_df['user_input'].tolist()\n",
        "    ground_truths = test_df['reference'].tolist()\n",
        "    \n",
        "    # Generate answers and contexts\n",
        "    answers = []\n",
        "    contexts = []\n",
        "\n",
        "    \n",
        "    # And modify each chain invocation to include project info:\n",
        "    for question in questions:\n",
        "        try:\n",
        "            result = chain.invoke(\n",
        "                {\"question\": question}, \n",
        "                metadata={\"revision_id\": \"default_chain_init\",\"project_name\": f\"ragas-{retriever_name}\"},\n",
        "            )\n",
        "            answers.append(result[\"response\"])\n",
        "            contexts.append(format_docs(result[\"context\"]))\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing question: {e}\")\n",
        "            answers.append(\"Error generating answer\")\n",
        "            contexts.append([\"Error retrieving context\"])\n",
        "    \n",
        "    # Calculate latency\n",
        "    end_time = time.time()\n",
        "    avg_latency = (end_time - start_time) / len(questions)\n",
        "    latency_tracking[retriever_name] = avg_latency\n",
        "    \n",
        "    # Create evaluation dataset\n",
        "    eval_data = {\n",
        "        \"question\": questions,\n",
        "        \"answer\": answers,\n",
        "        \"contexts\": contexts,\n",
        "        \"ground_truth\": ground_truths\n",
        "    }\n",
        "\n",
        "# Run Ragas evaluation\n",
        "    try:\n",
        "        from datasets import Dataset\n",
        "        \n",
        "        # Convert to Ragas dataset format\n",
        "        ragas_dataset = Dataset.from_dict(eval_data)\n",
        "        \n",
        "        ragas_result = evaluate(\n",
        "            dataset=ragas_dataset,  # Now using HuggingFace Dataset\n",
        "            metrics=metrics,\n",
        "            llm=generator_llm,\n",
        "            embeddings=generator_embeddings\n",
        "        )\n",
        "        \n",
        "        results[retriever_name] = {\n",
        "            \"scores\": ragas_result,\n",
        "            \"avg_latency\": avg_latency\n",
        "        }\n",
        "        \n",
        "        print(f\"{retriever_name} - Average Latency: {avg_latency:.2f}s\")\n",
        "        print(f\"{retriever_name} - Scores: {ragas_result}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating {retriever_name}: {e}\")\n",
        "        results[retriever_name] = {\"error\": str(e), \"avg_latency\": avg_latency}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Compile results and analysis\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_data = []\n",
        "# Replace the results compilation section with this:\n",
        "for retriever_name, result in results.items():\n",
        "    if \"error\" not in result:\n",
        "        scores = result[\"scores\"]\n",
        "        \n",
        "        # Try multiple ways to access the scores\n",
        "        score_dict = {}\n",
        "        \n",
        "        # Method 1: Direct attribute access\n",
        "        for metric_name in [\"context_precision\", \"context_recall\", \"answer_relevancy\", \"faithfulness\", \"answer_correctness\"]:\n",
        "            try:\n",
        "                score_dict[metric_name] = getattr(scores, metric_name, None)\n",
        "            except:\n",
        "                pass\n",
        "        \n",
        "        # Method 2: If it's a dict-like object\n",
        "        if hasattr(scores, 'items'):\n",
        "            score_dict.update(dict(scores.items()))\n",
        "        \n",
        "        # Method 3: If it has to_pandas method\n",
        "        if hasattr(scores, 'to_pandas'):\n",
        "            pandas_scores = scores.to_pandas()\n",
        "            if not pandas_scores.empty:\n",
        "                score_dict.update(pandas_scores.iloc[0].to_dict())\n",
        "        \n",
        "        print(f\"{retriever_name} extracted scores: {score_dict}\")\n",
        "        \n",
        "        row = {\n",
        "            \"Retriever\": retriever_name,\n",
        "            \"Context Precision\": score_dict.get(\"context_precision\", \"N/A\"),\n",
        "            \"Context Recall\": score_dict.get(\"context_recall\", \"N/A\"),\n",
        "            \"Answer Relevancy\": score_dict.get(\"answer_relevancy\", \"N/A\"),\n",
        "            \"Faithfulness\": score_dict.get(\"faithfulness\", \"N/A\"),\n",
        "            \"Answer Correctness\": score_dict.get(\"answer_correctness\", \"N/A\"),\n",
        "            \"Avg Latency (s)\": result[\"avg_latency\"]\n",
        "        }\n",
        "        comparison_data.append(row)\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Save results\n",
        "comparison_df.to_csv('retriever_evaluation_results.csv', index=False)\n",
        "\n",
        "# Analysis and Recommendations\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ANALYSIS & RECOMMENDATIONS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Cost Analysis (based on model usage)\n",
        "print(\"\\n📊 COST ANALYSIS:\")\n",
        "print(\"- Naive, BM25, Parent Document: Low cost (basic embedding + LLM)\")\n",
        "print(\"- Contextual Compression: Medium-High cost (additional Cohere Rerank API)\")\n",
        "print(\"- Multi-Query: Medium cost (additional LLM calls for query generation)\")\n",
        "print(\"- Ensemble: Medium cost (combines multiple retrieval methods)\")\n",
        "\n",
        "# Latency Analysis\n",
        "print(\"\\n⏱️ LATENCY ANALYSIS:\")\n",
        "latency_sorted = sorted(latency_tracking.items(), key=lambda x: x[1])\n",
        "for retriever, latency in latency_sorted:\n",
        "    print(f\"- {retriever}: {latency:.2f}s\")\n",
        "\n",
        "# Performance Analysis\n",
        "if comparison_data:\n",
        "    print(\"\\n🎯 PERFORMANCE ANALYSIS:\")\n",
        "    \n",
        "    # Find best performer for each metric\n",
        "    metrics_to_check = [\"Context Precision\", \"Context Recall\", \"Answer Relevancy\", \"Faithfulness\", \"Answer Correctness\"]\n",
        "    \n",
        "    for metric in metrics_to_check:\n",
        "        if metric in comparison_df.columns:\n",
        "            valid_scores = comparison_df[comparison_df[metric] != \"N/A\"][metric]\n",
        "            if not valid_scores.empty:\n",
        "                best_idx = valid_scores.astype(float).idxmax()\n",
        "                best_retriever = comparison_df.loc[best_idx, \"Retriever\"]\n",
        "                best_score = valid_scores.iloc[best_idx]\n",
        "                print(f\"- Best {metric}: {best_retriever} ({best_score})\")\n",
        "\n",
        "print(\"\\n📝 FINAL RECOMMENDATION:\")\n",
        "print(\"\"\"\n",
        "Based on the evaluation results, here's the analysis:\n",
        "\n",
        "1. **For Production Use**: Consider the balance of cost, latency, and performance\n",
        "2. **For High Accuracy**: Contextual Compression often performs best but at higher cost\n",
        "3. **For Fast Response**: Naive or BM25 retrievers offer good speed-performance trade-offs\n",
        "4. **For Comprehensive Coverage**: Multi-Query and Ensemble methods provide broader context\n",
        "\n",
        "Choose based on your specific requirements for accuracy vs. speed vs. cost.\n",
        "\"\"\")\n",
        "\n",
        "# Upload traces to LangSmith\n",
        "print(f\"\\n🔗 Evaluation traces have been uploaded to LangSmith project: ragas-evaluation-*\")\n",
        "print(\"Check your LangSmith dashboard for detailed cost and latency metrics.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "### 🏗️ Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against each other. \n",
        "You can use the loans or bills dataset.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Task 1: Retriever Evaluation with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### All Retriever Evaluation with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgDICngKXLGK"
      },
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "### Analysis & Observations:\n",
        "\n",
        "\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
